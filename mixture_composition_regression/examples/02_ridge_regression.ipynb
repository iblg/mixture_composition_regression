{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41d71c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from load_dipa_water_nacl_training_set import load_training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6924e5c2",
   "metadata": {},
   "source": [
    "### Load in a training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e62bf0",
   "metadata": {},
   "source": [
    "_For details on how to construct your own training set, consult load_dipa_water_nacl_training_set.py as a template._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6073b9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking samples in mixture water_dipa\n",
      "Checking samples in mixture water_nacl\n",
      "Checking samples in mixture all_three\n",
      "Checking samples in mixture None\n",
      "Sample water1 has a duplicate!\n",
      "Sample water2 has a duplicate!\n",
      "Sample water3 has a duplicate!\n",
      "Checking samples in mixture None\n",
      "Checking samples in mixture water_dipa_nacl\n"
     ]
    }
   ],
   "source": [
    "water_dipa_nacl, water_dipa, water_nacl = load_training_set(filepaths = [None, None, None, None]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b157f8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': ['water', 'dipa', 'nacl'],\n",
       " 'mw': [18.015, 101.19, 58.44],\n",
       " 'nu': [1, 1, 2]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "water_dipa_nacl.chem_properties #  examine some chemical properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f65397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "517b0ecf",
   "metadata": {},
   "source": [
    "Do a test-train split. This can be done randomly, or systematically. Here, we will do it systematically (non-randomly) using as a training set only DIPA-water and water-NaCl mixtures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "273063e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking samples in mixture None\n",
      "Sample water1 has a duplicate!\n",
      "Sample water2 has a duplicate!\n",
      "Sample water3 has a duplicate!\n",
      "Sample water1 has a duplicate!\n",
      "Sample water2 has a duplicate!\n",
      "Sample water3 has a duplicate!\n",
      "Sample 5M has a duplicate!\n",
      "Sample 5M_2 has a duplicate!\n",
      "Sample 2M has a duplicate!\n",
      "Sample 2M_2 has a duplicate!\n",
      "Sample 4M has a duplicate!\n",
      "Sample 4M_2 has a duplicate!\n",
      "Sample a2 has a duplicate!\n",
      "Sample a2a has a duplicate!\n",
      "Sample a3 has a duplicate!\n",
      "Sample a4 has a duplicate!\n",
      "Checking samples in mixture None\n"
     ]
    }
   ],
   "source": [
    "mix_train = water_dipa + water_nacl \n",
    "\n",
    "# filter the water-dipa-nacl set to only include samples that contain all three \n",
    "mix_test = water_dipa_nacl.filter({'water': [10 ** -5, 1], 'nacl': [10 ** -5, 1], 'dipa': [10 ** -5, 1]}) \n",
    "# Some print-outs are currently expected as the code checks for samples that exist in both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68683601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "lbounds = [800, 2500] # set global bounds on your spectrum. \n",
    "\n",
    "# Set the number of windows to split your spectral range into.\n",
    "# if nwindows = [1, 10], the code will first try to use the entire spectral range as a training set,\n",
    "# then will split the range into 10 smaller ranges and try them sequentially.\n",
    "nwindows = [10] \n",
    "\n",
    "sc = 'neg_mean_absolute_error' # scoring metric.\n",
    "random_state = 42 # a replicable random state. \n",
    "tts_size = 0.25 # the fraction of samples to use as testing in the test-train split.\n",
    "metric = mean_absolute_error # the scoring metric to use\n",
    "metric_label = 'MAE' # for plotting and printouts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4653695d",
   "metadata": {},
   "source": [
    "Create your search plan. We import GridSearchCV, an exhaustive search method that will try all combos of parameters until it finds the best-working fit. \n",
    "\n",
    "We will use Ridge regression, a type of machine learning regression.\n",
    "\n",
    "The only adjustable parameter that we are telling the program to search over is the alpha parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "040ae528",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_search_plan = GridSearchCV(\n",
    "    Ridge(), {'alpha': np.logspace(-7, 7, 14)}, scoring=sc, cv=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41bfab5",
   "metadata": {},
   "source": [
    "Create a list of search plans. Each is typically a different kind of machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cd6b011",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [ridge_search_plan] # currently, only one search plan is in the list. But you could include more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d586727a",
   "metadata": {},
   "source": [
    "Perform the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ecb2710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in cv_and_wavel\n",
      "Running analysis splitting interval into 10 windows.\n",
      "Running analysis on Ridge()\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 70 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n70 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/ianbillinge/miniconda3/envs/mcr/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/ianbillinge/miniconda3/envs/mcr/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py\", line 1126, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ianbillinge/miniconda3/envs/mcr/lib/python3.11/site-packages/sklearn/base.py\", line 584, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ianbillinge/miniconda3/envs/mcr/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 1106, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"/Users/ianbillinge/miniconda3/envs/mcr/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 921, in check_array\n    _assert_all_finite(\n  File \"/Users/ianbillinge/miniconda3/envs/mcr/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 161, in _assert_all_finite\n    raise ValueError(msg_err)\nValueError: Input X contains NaN.\nRidge does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmixture_composition_regression\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cv_on_model_and_wavelength\n\u001b[0;32m----> 3\u001b[0m viable_models, best_model, y, X \u001b[38;5;241m=\u001b[39m \u001b[43mcv_on_model_and_wavelength\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmix_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnwindows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_chem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwater\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmix_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtts_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtts_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtts_random_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5E-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43ml_bounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplot_comparison\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplot_comparison_savefile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./plots/axes_train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/mixture_composition_regression/mixture_composition_regression/cross_validation.py:107\u001b[0m, in \u001b[0;36mcv_on_model_and_wavelength\u001b[0;34m(m, nwindows, models, l_bounds, target_chem, tts_test_size, tts_random_state, tolerance, metric, metric_label, test_data, plot_comparison, plot_comparison_savefile)\u001b[0m\n\u001b[1;32m    104\u001b[0m     y_train, X_train \u001b[38;5;241m=\u001b[39m get_Xy_2(m, lbounds\u001b[38;5;241m=\u001b[39ml_window, target_chem\u001b[38;5;241m=\u001b[39mtarget_chem)  \u001b[38;5;66;03m# get y, X data\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     y_test, X_test \u001b[38;5;241m=\u001b[39m get_Xy_2(test_data, lbounds\u001b[38;5;241m=\u001b[39ml_window, target_chem\u001b[38;5;241m=\u001b[39mtarget_chem)\n\u001b[0;32m--> 107\u001b[0m model_instance \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m                           \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# model instance is the model with optimized params by gridsearch CV\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m    112\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model_instance\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/miniconda3/envs/mcr/lib/python3.11/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/mcr/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1387\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mcr/lib/python3.11/site-packages/sklearn/model_selection/_search.py:851\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    846\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    847\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    848\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[1;32m    849\u001b[0m     )\n\u001b[0;32m--> 851\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[0;32m~/miniconda3/envs/mcr/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[1;32m    361\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    364\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    366\u001b[0m     )\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    370\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    371\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    377\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 70 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n70 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/ianbillinge/miniconda3/envs/mcr/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/ianbillinge/miniconda3/envs/mcr/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py\", line 1126, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ianbillinge/miniconda3/envs/mcr/lib/python3.11/site-packages/sklearn/base.py\", line 584, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/ianbillinge/miniconda3/envs/mcr/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 1106, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"/Users/ianbillinge/miniconda3/envs/mcr/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 921, in check_array\n    _assert_all_finite(\n  File \"/Users/ianbillinge/miniconda3/envs/mcr/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 161, in _assert_all_finite\n    raise ValueError(msg_err)\nValueError: Input X contains NaN.\nRidge does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n"
     ]
    }
   ],
   "source": [
    "from mixture_composition_regression.cross_validation import cv_on_model_and_wavelength\n",
    "\n",
    "viable_models, best_model, y, X = cv_on_model_and_wavelength(\n",
    "    mix_train,\n",
    "    nwindows, \n",
    "    models,\n",
    "    target_chem='water',\n",
    "    test_data=mix_test,\n",
    "    tts_test_size=tts_size,\n",
    "    tts_random_state=random_state,\n",
    "    tolerance=5E-3,\n",
    "    metric=metric,\n",
    "    metric_label=metric_label,\n",
    "    l_bounds=lbounds,\n",
    "    plot_comparison=True,\n",
    "    plot_comparison_savefile='./plots/axes_train'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea77cf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebbf518",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
