{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d808f483-a183-416a-afa3-bc224068205f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata, interpn\n",
    "from mixture_composition_regression.examples.cellulose_example.helper_functions import *\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18a61b0",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "94d0b5d8-71f7-420e-abce-b52fcbbb834f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor_files = ['cellulose_predictor.pkl', 'hemi_predictor.pkl', 'lignin_predictor.pkl', 'rot_predictor.pkl']\n",
    "predictor_files = ['./trained_models/' + p for p in predictor_files]\n",
    "predictor_metadata_files = [p.split('.pkl')[0] + '_meta.txt' for p in predictor_files] \n",
    "predictor_uncertainty_files = [p.split('.pkl')[0] + '_uncertainty.txt' for p in predictor_files]\n",
    "\n",
    "ranges = read_range_files(predictor_metadata_files)\n",
    "predictors = read_predictor_files(predictor_files)\n",
    "uncertainties = read_uncertainty_files(predictor_uncertainty_files)\n",
    "uncertainties = [i[0] for i in uncertainties]\n",
    "\n",
    "containers = [[j,i] for i, j in zip(ranges, predictors)]\n",
    "c_container = containers[0]\n",
    "h_container = containers[1]\n",
    "l_container = containers[2]\n",
    "r_container = containers[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b349b8-8c1e-457f-b876-6e6bdafa54e2",
   "metadata": {},
   "source": [
    "#### Get an appropriate x grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e2022d3c-690f-4209-8ce8-8e5a1a4e67f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fn = '/Users/ianbillinge/dev/mixture_composition_regression/mixture_composition_regression/examples/cellulose_example/expanded_training_set/lignocellulose_expanded_training_set/xgrid.csv'\n",
    "xgrid = np.loadtxt(fn, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283492f2",
   "metadata": {},
   "source": [
    "### Read in data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "36dbe5d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p = Path().resolve() / 'expanded_training_set' # define the path to the current data\n",
    "extensions = ['.CSV'] #define list of extensions you want to glob\n",
    "files = [path for path in p.rglob('*') if path.suffix in extensions] #create a list of paths containing that extension\n",
    "\n",
    "data = [pd.read_csv(file, names=['x', 'y']) for file in files] #read files and trim spectral range\n",
    "data = [df.where(df['x'] > xgrid.min()).where(df['x'] < xgrid.max()).dropna() for df in data]\n",
    "\n",
    "\n",
    "data2 = []### for some reason, this does not work inplace -- a new list must be created\n",
    "for df in data:\n",
    "    yinterp = griddata(df['x'].values, df['y'].values, xgrid).T\n",
    "    data2.append(pd.DataFrame([xgrid, yinterp]).T.dropna())\n",
    "\n",
    "data2 = [df.rename(columns={df.columns[0]:'x',df.columns[1]:'y'}) for df in data2]\n",
    "\n",
    "#overwrite data\n",
    "data = data2\n",
    "# del(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "54dac2b2-87a6-481b-94b2-0eee1d30287e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_predictions_to_file(sample_names, l_list, c_list, h_list, r_list, uncertainties):\n",
    "    print('We are in save')\n",
    "    df = pd.DataFrame([sample_names, l_list, c_list, h_list, r_list],\n",
    "                      # columns = ['sample', 'lignin', 'cellulose', 'hemicellulose', 'white rot']\n",
    "                      )\n",
    "    df = df.T\n",
    "    df = df.rename(columns={df.columns[0]:'sample', df.columns[1]:'lignin', df.columns[2]: 'cellulose',\n",
    "                            df.columns[3]: 'hemicellulose', df.columns[4]: 'white rot'})\n",
    "    df=df.dropna()\n",
    "    regressands = ['lignin', 'cellulose', 'hemicellulose', 'white rot']\n",
    "    for i, dy in zip(regressands, uncertainties):\n",
    "        df['uncertainty' + i] = 2 * float(dy) * np.ones(len(c_list))\n",
    "    # df['total_predicted_weight_fraction'] = df['lignin'] + df['cellulose']+ df['hemicellulose'] + df['white rot']\n",
    "    df.to_csv('results.csv')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 21-1\n",
      "Sample: Day 21-1\n",
      "Day 23-3\n",
      "Sample: Day 23-3\n",
      "Day 23-2\n",
      "Sample: Day 23-2\n",
      "Day 21-2\n",
      "Sample: Day 21-2\n",
      "Day 23-1\n",
      "Sample: Day 23-1\n",
      "Day 21-3\n",
      "Sample: Day 21-3\n",
      "Day 18-3\n",
      "Sample: Day 18-3\n",
      "Day 18-2\n",
      "Sample: Day 18-2\n",
      "Day 18-1\n",
      "Sample: Day 18-1\n",
      "Day 20-2\n",
      "Sample: Day 20-2\n",
      "Day 20-3\n",
      "Sample: Day 20-3\n",
      "Day 22-1\n",
      "Sample: Day 22-1\n",
      "Day 22-3\n",
      "Sample: Day 22-3\n",
      "Day 20-1\n",
      "Sample: Day 20-1\n",
      "Day 22-2\n",
      "Sample: Day 22-2\n",
      "Day 19-1\n",
      "Sample: Day 19-1\n",
      "Day 24-1\n",
      "Sample: Day 24-1\n",
      "Day 24-3\n",
      "Sample: Day 24-3\n",
      "Day 19-3\n",
      "Sample: Day 19-3\n",
      "Day 22-5\n",
      "Sample: Day 22-5\n",
      "Day 22-4\n",
      "Sample: Day 22-4\n",
      "Day 19-2\n",
      "Sample: Day 19-2\n",
      "Day 24-2\n",
      "Sample: Day 24-2\n",
      "Day 28-2\n",
      "Sample: Day 28-2\n",
      "Day 15-2\n",
      "Sample: Day 15-2\n",
      "Day 15-3\n",
      "Sample: Day 15-3\n",
      "Day 17-1\n",
      "Sample: Day 17-1\n",
      "Day 28-3\n",
      "Sample: Day 28-3\n",
      "Day 28-1\n",
      "Sample: Day 28-1\n",
      "Day 17-3\n",
      "Sample: Day 17-3\n",
      "Day 15-1\n",
      "Sample: Day 15-1\n",
      "Day 17-2\n",
      "Sample: Day 17-2\n",
      "Day 13-2\n",
      "Sample: Day 13-2\n",
      "CF11-3\n",
      "Sample: CF11-3\n",
      "Day 15-4\n",
      "Sample: Day 15-4\n",
      "CF11-2\n",
      "Sample: CF11-2\n",
      "Day 11-1\n",
      "Sample: Day 11-1\n",
      "Day 13-3\n",
      "Sample: Day 13-3\n",
      "Day 13-1\n",
      "Sample: Day 13-1\n",
      "Day 11-3\n",
      "Sample: Day 11-3\n",
      "CF11-1\n",
      "Sample: CF11-1\n",
      "Day 11-2\n",
      "Sample: Day 11-2\n",
      "Day 14-1\n",
      "Sample: Day 14-1\n",
      "Day 16-3\n",
      "Sample: Day 16-3\n",
      "Day 10-5\n",
      "Sample: Day 10-5\n",
      "Day 10-4\n",
      "Sample: Day 10-4\n",
      "Day 16-2\n",
      "Sample: Day 16-2\n",
      "Day 15-1-new\n",
      "Sample: Day 15-1-new\n",
      "Day 14-2\n",
      "Sample: Day 14-2\n",
      "Day 12-4\n",
      "Sample: Day 12-4\n",
      "Day 12-5\n",
      "Sample: Day 12-5\n",
      "Day 16-1\n",
      "Sample: Day 16-1\n",
      "Day 14-3\n",
      "Sample: Day 14-3\n",
      "Day 10-3\n",
      "Sample: Day 10-3\n",
      "Day 12-1\n",
      "Sample: Day 12-1\n",
      "Day 10-2\n",
      "Sample: Day 10-2\n",
      "Day 16-4\n",
      "Sample: Day 16-4\n",
      "CF10-1\n",
      "Sample: CF10-1\n",
      "CF10-3\n",
      "Sample: CF10-3\n",
      "Day 14-4\n",
      "Sample: Day 14-4\n",
      "Day 12-2\n",
      "Sample: Day 12-2\n",
      "Day 12-3\n",
      "Sample: Day 12-3\n",
      "Day 10-1\n",
      "Sample: Day 10-1\n",
      "Day 14-5\n",
      "Sample: Day 14-5\n",
      "CF10-2\n",
      "Sample: CF10-2\n",
      "\n",
      "could not convert string to float: '621fddf75404f8c60c2ef0bbed5ab571b104762e refs/remotes/origin/main'\n",
      "\n",
      "'utf-8' codec can't decode byte 0xb9 in position 12: invalid start byte\n",
      "\n",
      "'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "\n",
      "Error tokenizing data. C error: Expected 1 fields in line 4, saw 2\n",
      "\n",
      "\n",
      "Error tokenizing data. C error: Expected 1 fields in line 3, saw 2\n",
      "\n",
      "\n",
      "Error tokenizing data. C error: Expected 1 fields in line 8, saw 2\n",
      "\n",
      "\n",
      "Error tokenizing data. C error: Expected 1 fields in line 10, saw 2\n",
      "\n",
      "\n",
      "Error tokenizing data. C error: Expected 1 fields in line 16, saw 2\n",
      "\n",
      "\n",
      "Error tokenizing data. C error: Expected 1 fields in line 7, saw 2\n",
      "\n",
      "\n",
      "Error tokenizing data. C error: Expected 1 fields in line 5, saw 2\n",
      "\n",
      "\n",
      "Error tokenizing data. C error: Expected 1 fields in line 6, saw 2\n",
      "\n",
      "\n",
      "Error tokenizing data. C error: Expected 1 fields in line 8, saw 2\n",
      "\n",
      "\n",
      "Error tokenizing data. C error: Expected 1 fields in line 9, saw 2\n",
      "\n",
      "\n",
      "Error tokenizing data. C error: Expected 1 fields in line 4, saw 2\n",
      "\n",
      "\n",
      "Error tokenizing data. C error: Expected 1 fields in line 6, saw 2\n",
      "\n",
      "We are in save\n"
     ]
    }
   ],
   "source": [
    "files = p.rglob('*.CSV' and '*-*')\n",
    "fpaths = [str(file) for file in list(files)]\n",
    "c_list, h_list, l_list, r_list = [], [], [], []\n",
    "sample_names = [i.split('.')[0].split('/')[-1] for i in fpaths]\n",
    "regressand = 'da'\n",
    "to_print=False\n",
    "for f, n in zip(fpaths, sample_names):\n",
    "    print(n)\n",
    "    try:\n",
    "        l = predict_on_test_csvs(f, l_container, regressand, 'lignin', sample_name=n,\n",
    "                                 printres=to_print, xgrid=xgrid, print_sample=True)\n",
    "        l_list.append(l)\n",
    "\n",
    "        c = predict_on_test_csvs(f, c_container, regressand, 'cellulose',sample_name=n,\n",
    "                                 xgrid = xgrid,\n",
    "                                 printres=to_print)\n",
    "        c_list.append(c)\n",
    "\n",
    "        h = predict_on_test_csvs(f, h_container, regressand, 'hemicellulose',sample_name=n,\n",
    "                                 xgrid = xgrid, printres=to_print)\n",
    "        h_list.append(h)\n",
    "\n",
    "        r = predict_on_test_csvs(f, r_container, regressand, 'rot', sample_name=n,\n",
    "                                 xgrid = xgrid, printres=to_print)\n",
    "        r_list.append(h)\n",
    "    except ValueError as ve:\n",
    "        print(ve)\n",
    "        # print('This occurred while working on {}'.format(n))\n",
    "\n",
    "df = save_predictions_to_file(sample_names, l_list, c_list, h_list, r_list, uncertainties)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
